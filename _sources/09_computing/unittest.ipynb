{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit Testing\n",
    "\n",
    "[Unit testing](https://en.wikipedia.org/wiki/Unit_testing) is a method of testing your code by writing tests for individual functions.\n",
    "\n",
    "Let's say you write a package which provides a function.  You want to convince someone (espeically yourself) that the function works as intended.  An excellent way to do this is to write unit tests, which (assuming they pass) demonstrate that your function does what is supposed to do, at least for the situations you test.\n",
    "\n",
    "## unittest package\n",
    "\n",
    "[unittest](https://docs.python.org/3.8/library/unittest.html) is a built-in package which provides unit testing capabilities.\n",
    "\n",
    "Generally, you define classes that inherit from `unittest.TestCase`.  Then you can add methods which test different functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "class TestStringMethods(unittest.TestCase):\n",
    "\n",
    "    def test_upper(self):\n",
    "        self.assertEqual('foo'.upper(), 'FOO')\n",
    "\n",
    "    def test_isupper(self):\n",
    "        self.assertTrue('FOO'.isupper())\n",
    "        self.assertFalse('Foo'.isupper())\n",
    "\n",
    "    def test_split(self):\n",
    "        s = 'hello world'\n",
    "        self.assertEqual(s.split(), ['hello', 'world'])\n",
    "        # check that s.split fails when the separator is not a string\n",
    "        with self.assertRaises(TypeError):\n",
    "            s.split(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then run our tests using `unittest.main()` (the arguments below are passed in so we can run in Jupyter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "...\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.007s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7f36a8f9b910>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using floating-point numbers, use `assertAlmostEqual` instead of `assertEqual` to test for numerical equality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19999999999999996"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.2 - 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestFloatArithmetic(unittest.TestCase):\n",
    "    \n",
    "    def test_approx(self):\n",
    "        self.assertAlmostEqual(1.2 - 1.0, 0.2)\n",
    "        \n",
    "    def test_exact(self):\n",
    "        self.assertEqual(1.2 - 1.0, 0.2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".F...\n",
      "======================================================================\n",
      "FAIL: test_exact (__main__.TestFloatArithmetic)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-4-bc6eae005c7e>\", line 7, in test_exact\n",
      "    self.assertEqual(1.2 - 1.0, 0.2 )\n",
      "AssertionError: 0.19999999999999996 != 0.2\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 5 tests in 0.009s\n",
      "\n",
      "FAILED (failures=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7f36a871d910>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running from command line\n",
    "\n",
    "The more common way to run unit tests is to have them in a test folder or file `test.py`.  You can then run tests using\n",
    "\n",
    "```bash\n",
    "python -m unittest test.py\n",
    "```\n",
    "\n",
    "Or use `pytest` via\n",
    "```bash\n",
    "pytest test.py\n",
    "```\n",
    "\n",
    "[pytest](https://docs.pytest.org/en/stable/) is another Python testing framework - it is compatible with `unittest`, and has additional funcitonality which we aren't going to cover."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test-Driven Development\n",
    "\n",
    "You don't need to wait to implement everything in order to write your tests.  Writing your tests first is called **test-driven development**.  One advantage of test-driven development is that you'll know when you have succeeded in your implementation, since all your tests will pass.\n",
    "\n",
    "Let's consider a suite of tests that would test a `power_method` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.ones((5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestPowerMethod(unittest.TestCase):\n",
    "    \n",
    "    def test_eigenpair(self):\n",
    "        n = 5\n",
    "        A = np.random.randn((n,n))\n",
    "        A = A + A.T # make symmetric\n",
    "        lam, v = powermethod(A)\n",
    "        v2 = A @ v\n",
    "        self.assertAlmostEqual(np.linalg.norm(v2 - lam * v), 0)\n",
    "    \n",
    "    def test_norm1(self):\n",
    "        n = 5\n",
    "        A = np.random.randn((n,n))\n",
    "        A = A + A.T # make symmetric\n",
    "        lam, v = powermethod(A)\n",
    "        self.assertAlmostEqual(np.linalg.norm(v), 1)\n",
    "    \n",
    "    def test_rank1(self):\n",
    "        n = 5\n",
    "        A = np.ones((n,n))\n",
    "        lam, v = powermethod(A)\n",
    "        # check that v is close to constant function.\n",
    "        self.assertAlmostEqual(np.linalg.norm(v - np.ones(n)/np.sqrt(n)), 0)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Implement a function `powermethod` which satisfies the above tests (using the Power method algorithm, of course)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Examples\n",
    "\n",
    "* Check out `test.py` in each homework assignment, which is used for autograding.\n",
    "* You can find another example in the repository [`python-packages`](https://github.com/caam37830/python-packages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Integration\n",
    "\n",
    "[Continuous Integration](https://en.wikipedia.org/wiki/Continuous_integration), or CI, is the practice of automatically building code and running tests *continuously*.  Continuously in this case generally means any time changes are made, which might be multiple times a day.\n",
    "\n",
    "The advantage of CI is that when you make changes to your code, you quickly find out if there are problems that need to be solved if your tests fail.  You can run these tests before merging branches in your git repository, making sure that checks pass.\n",
    "\n",
    "[GitHub actions](https://docs.github.com/en/free-pro-team@latest/actions/learn-github-actions/introduction-to-github-actions) is one way of implementing CI.  This is what we are using in this class - you can find an example in the [`python-packages` repository](https://github.com/caam37830/python-packages/blob/main/.github/workflows/push.yml).\n",
    "\n",
    "Another popular option which you may see in open source software is [Travis-CI](https://travis-ci.org/).  \n",
    "\n",
    "Both these platforms are configured using a `*.yml` file.  See the [`python-packages` repository](https://github.com/caam37830/python-packages/blob/main/.github/workflows/push.yml) for an example.\n",
    "\n",
    "You can do more than just run unit tests using CI, such as running integration tests, verifying that data analyses don't change, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pycourse)",
   "language": "python",
   "name": "pycourse"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
